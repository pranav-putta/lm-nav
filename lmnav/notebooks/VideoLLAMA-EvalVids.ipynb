{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c743f7-bd59-44b9-85cd-b93864a5006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-24 21:28:32,437] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from video_llama.common.config import Config\n",
    "from video_llama.common.dist_utils import get_rank\n",
    "from video_llama.common.registry import registry\n",
    "from video_llama.conversation.conversation_video import Chat, Conversation, default_conversation,SeparatorStyle,conv_llava_llama_2\n",
    "\n",
    "#%%\n",
    "# imports modules for registration\n",
    "from video_llama.datasets.builders import *\n",
    "from video_llama.models import *\n",
    "from video_llama.processors import *\n",
    "from video_llama.runners import *\n",
    "from video_llama.tasks import *\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/srv/flash1/pputta7/projects/lm-nav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3909b8c3-4fab-4e0e-a5b9-b64a58901a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cfg_path = \"Video-LLaMA/eval_configs/video_llama_eval_only_vl.yaml\"\n",
    "        self.model_type = \"llama_v2\"\n",
    "        self.gpu_id = 0\n",
    "        self.options = []\n",
    "\n",
    "args = Args()\n",
    "cfg = Config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4797c8e9-7af0-47dc-a24a-e8c72e9ad619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593b6cb869124a72a6a635657eda1a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load first Checkpoint: /srv/flash1/pputta7/data/models/Video-LLaMA-2-13B-Finetuned/VL_LLaMA_2_13B_Finetuned.pth\n",
      "Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "model_config = cfg.model_cfg\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "model.eval()\n",
    "vis_processor_cfg = cfg.datasets_cfg.webvid.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n",
    "print('Initialization Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca517e10-6a0f-4bc1-9809-56c2ca2ee824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/flash1/pputta7/projects/lm-nav/experiments/lmnav-1env/bc/lora+clip+max_trajectory_length=200+batch_size=9/eval/ckpt.56.pth/videos/episode=17-ckpt=56-.mp4\n"
     ]
    }
   ],
   "source": [
    "chat_state = conv_llava_llama_2.copy()\n",
    "chat_state.system =  \"You are a navigational robot with egocentric vision that navigates from a start to goal point.\"\n",
    "img_list = []\n",
    "llm_message = chat.upload_video_without_audio(\"/srv/flash1/pputta7/projects/lm-nav/experiments/lmnav-1env/bc/lora+clip+max_trajectory_length=200+batch_size=9/eval/ckpt.56.pth/videos/episode=17-ckpt=56-.mp4\", chat_state, img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2475bf02-a375-492f-9543-91a9e7f77663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a navigational robot with egocentric vision that navigates from a start to goal point.\n",
      "<</SYS>>\n",
      "\n",
      "<Video><ImageHere></Video> The video contains 8 frames sampled at 0.0, 3.9, 7.8, 11.8, 15.7, 19.6, 23.6, 27.5 seconds.  Describe what happened during the route in detail. [/INST] First, the video shows a room with a green and white wall, a yellow and white couch, and a white and brown chair. Next, the video shows a living room with a yellow and white couch, a white and brown chair, and a white and brown couch. Then, the video shows a room with a yellow and white couch, a white and brown couch, and a white and brown chair. After that, the video shows a living room with a yellow and white couch, a white and brown couch, and a white and brown chair. The video then shows a room with a yellow and white couch, a white and brown couch, and a white and brown chair. Next, the video shows a room with a yellow and white couch, a white and brown couch, and a white and brown chair. Then, the video shows a living room with a yellow and white couch, a white and brown couch, and a white and brown chair. Finally, the video shows a room with a yellow and white couch, a white and brown couch, and a white and brown chair. Throughout the video, the camera captures the various objects and colors in the room, such as the yellow and white couch, the white and brown chair, and the green and white wall. </s>\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Describe what happened during the route in detail.\"\n",
    "chat.ask(user_message, chat_state)\n",
    "llm_message = chat.answer(conv=chat_state,\n",
    "                              img_list=img_list,\n",
    "                              num_beams=1,\n",
    "                              temperature=0.6,\n",
    "                              max_new_tokens=600,\n",
    "                              max_length=2000)[0]\n",
    "print(chat_state.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb3411-ad22-44ef-a6e7-f0989c44179e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
