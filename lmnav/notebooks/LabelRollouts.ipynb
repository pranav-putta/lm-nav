{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed458d8a-fa6c-476f-b4e9-1f4a02f88955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [setup]\n",
    "import os\n",
    "from typing import TYPE_CHECKING, Union, cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import habitat\n",
    "from habitat.config.default_structured_configs import (\n",
    "    CollisionsMeasurementConfig,\n",
    "    FogOfWarConfig,\n",
    "    TopDownMapMeasurementConfig,\n",
    ")\n",
    "from habitat.core.agent import Agent\n",
    "from habitat.tasks.nav.nav import NavigationEpisode, NavigationGoal\n",
    "from habitat.tasks.nav.shortest_path_follower import ShortestPathFollower\n",
    "from habitat.utils.visualizations import maps\n",
    "from habitat.utils.visualizations.utils import (\n",
    "    images_to_video,\n",
    "    observations_to_image,\n",
    "    overlay_frame,\n",
    ")\n",
    "from habitat.core.registry import registry\n",
    "from habitat_sim.utils import viz_utils as vut\n",
    "from lmnav.config.default import get_config\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional\n",
    "\n",
    "from habitat.config import read_write\n",
    "from habitat.core.dataset import ALL_SCENES_MASK, Dataset\n",
    "from habitat.core.registry import registry\n",
    "from habitat.tasks.nav.nav import (\n",
    "    NavigationEpisode,\n",
    "    NavigationGoal,\n",
    "    ShortestPathPoint,\n",
    ")\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from omegaconf import DictConfig\n",
    "\n",
    "# Quiet the Habitat simulator logging\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from habitat.core.simulator import Observations\n",
    "    from habitat.sims.habitat_simulator.habitat_simulator import HabitatSim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ef9b7d-35e3-4f39-ba16-3a93fa4bfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_SCENES_PATH_FIELD = \"content_scenes_path\"\n",
    "DEFAULT_SCENE_PATH_PREFIX = \"data/scene_datasets/\"\n",
    "\n",
    "@registry.register_dataset(name=\"OfflineDataset-v1\")\n",
    "class OfflineDatasetV1(Dataset):\n",
    "    r\"\"\"Class inherited from Dataset that loads Point Navigation dataset.\"\"\"\n",
    "\n",
    "    episodes: List[NavigationEpisode]\n",
    "    content_scenes_path: str = \"{data_path}/content/{scene}.json.gz\"\n",
    "\n",
    "    @staticmethod\n",
    "    def check_config_paths_exist(config: \"DictConfig\") -> bool:\n",
    "        return os.path.exists(\n",
    "            config.data_path.format(split=config.split)\n",
    "        ) and os.path.exists(config.scenes_dir)\n",
    "\n",
    "    @classmethod\n",
    "    def get_scenes_to_load(cls, config: \"DictConfig\") -> List[str]:\n",
    "        r\"\"\"Return list of scene ids for which dataset has separate files with\n",
    "        episodes.\n",
    "        \"\"\"\n",
    "        dataset_dir = os.path.dirname(\n",
    "            config.data_path.format(split=config.split)\n",
    "        )\n",
    "        if not cls.check_config_paths_exist(config):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find dataset file `{dataset_dir}`\"\n",
    "            )\n",
    "\n",
    "        cfg = config.copy()\n",
    "        with read_write(cfg):\n",
    "            cfg.content_scenes = []\n",
    "            dataset = cls(cfg)\n",
    "            has_individual_scene_files = os.path.exists(\n",
    "                dataset.content_scenes_path.split(\"{scene}\")[0].format(\n",
    "                    data_path=dataset_dir\n",
    "                )\n",
    "            )\n",
    "            if has_individual_scene_files:\n",
    "                return cls._get_scenes_from_folder(\n",
    "                    content_scenes_path=dataset.content_scenes_path,\n",
    "                    dataset_dir=dataset_dir,\n",
    "                )\n",
    "            else:\n",
    "                # Load the full dataset, things are not split into separate files\n",
    "                cfg.content_scenes = [ALL_SCENES_MASK]\n",
    "                dataset = cls(cfg)\n",
    "                return list(map(cls.scene_from_scene_path, dataset.scene_ids))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_scenes_from_folder(\n",
    "        content_scenes_path: str, dataset_dir: str\n",
    "    ) -> List[str]:\n",
    "        scenes: List[str] = []\n",
    "        content_dir = content_scenes_path.split(\"{scene}\")[0]\n",
    "        scene_dataset_ext = content_scenes_path.split(\"{scene}\")[1]\n",
    "        content_dir = content_dir.format(data_path=dataset_dir)\n",
    "        if not os.path.exists(content_dir):\n",
    "            return scenes\n",
    "\n",
    "        for filename in os.listdir(content_dir):\n",
    "            if filename.endswith(scene_dataset_ext):\n",
    "                scene = filename[: -len(scene_dataset_ext)]\n",
    "                scenes.append(scene)\n",
    "        scenes.sort()\n",
    "        return scenes\n",
    "\n",
    "    def _load_from_file(self, fname: str, scenes_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Load the data from a file into `self.episodes`. This can load `.pickle`\n",
    "        or `.json.gz` file formats.\n",
    "        \"\"\"\n",
    "\n",
    "        if fname.endswith(\".pickle\"):\n",
    "            # NOTE: not implemented for pointnav\n",
    "            with open(fname, \"rb\") as f:\n",
    "                self.from_binary(pickle.load(f), scenes_dir=scenes_dir)\n",
    "        else:\n",
    "            with gzip.open(fname, \"rt\") as f:\n",
    "                self.from_json(f.read(), scenes_dir=scenes_dir)\n",
    "\n",
    "    def __init__(self, config: Optional[\"DictConfig\"] = None, directory=None) -> None:\n",
    "        self.episodes = []\n",
    "\n",
    "        if config is None:\n",
    "            return\n",
    "\n",
    "        datasetfile_path = config.data_path.format(split=config.split)\n",
    "\n",
    "        self._load_from_file(datasetfile_path, config.scenes_dir)\n",
    "\n",
    "        # Read separate file for each scene\n",
    "        dataset_dir = os.path.dirname(datasetfile_path)\n",
    "        has_individual_scene_files = os.path.exists(\n",
    "            self.content_scenes_path.split(\"{scene}\")[0].format(\n",
    "                data_path=dataset_dir\n",
    "            )\n",
    "        )\n",
    "        if has_individual_scene_files:\n",
    "            scenes = config.content_scenes\n",
    "            if ALL_SCENES_MASK in scenes:\n",
    "                scenes = self._get_scenes_from_folder(\n",
    "                    content_scenes_path=self.content_scenes_path,\n",
    "                    dataset_dir=dataset_dir,\n",
    "                )\n",
    "\n",
    "            for scene in scenes:\n",
    "                scene_filename = self.content_scenes_path.format(\n",
    "                    data_path=dataset_dir, scene=scene\n",
    "                )\n",
    "\n",
    "                self._load_from_file(scene_filename, config.scenes_dir)\n",
    "\n",
    "        else:\n",
    "            self.episodes = list(\n",
    "                filter(self.build_content_scenes_filter(config), self.episodes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def to_binary(self) -> Dict[str, Any]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def from_binary(\n",
    "        self, data_dict: Dict[str, Any], scenes_dir: Optional[str] = None\n",
    "    ) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def from_json(\n",
    "        self, json_str: str, scenes_dir: Optional[str] = None\n",
    "    ) -> None:\n",
    "        deserialized = json.loads(json_str)\n",
    "        if CONTENT_SCENES_PATH_FIELD in deserialized:\n",
    "            self.content_scenes_path = deserialized[CONTENT_SCENES_PATH_FIELD]\n",
    "\n",
    "        for episode in deserialized[\"episodes\"]:\n",
    "            episode = NavigationEpisode(**episode)\n",
    "\n",
    "            if scenes_dir is not None:\n",
    "                if episode.scene_id.startswith(DEFAULT_SCENE_PATH_PREFIX):\n",
    "                    episode.scene_id = episode.scene_id[\n",
    "                        len(DEFAULT_SCENE_PATH_PREFIX) :\n",
    "                    ]\n",
    "\n",
    "                episode.scene_id = os.path.join(scenes_dir, episode.scene_id)\n",
    "\n",
    "            for g_index, goal in enumerate(episode.goals):\n",
    "                episode.goals[g_index] = NavigationGoal(**goal)\n",
    "            if episode.shortest_paths is not None:\n",
    "                for path in episode.shortest_paths:\n",
    "                    for p_index, point in enumerate(path):\n",
    "                        path[p_index] = ShortestPathPoint(**point)\n",
    "            self.episodes.append(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de1fd9fa-3989-4570-bacf-ffa82257bf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 23:03:33,093 Initializing dataset OfflineDataset-v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/srv/flash1/pputta7/projects/lm-nav\")\n",
    "config = get_config(\"train/nav_llama/1env_karmesh/bc/lora+clip+karmesh\")\n",
    "with habitat.config.read_write(config):\n",
    "        config.habitat.task.measurements.update(\n",
    "            {\n",
    "                \"top_down_map\": TopDownMapMeasurementConfig(\n",
    "                    map_padding=0,\n",
    "                    map_resolution=128,\n",
    "                    draw_source=False,\n",
    "                    draw_border=False,\n",
    "                    draw_shortest_path=False,\n",
    "                    draw_view_points=False,\n",
    "                    draw_goal_positions=False,\n",
    "                    draw_goal_aabbs=False,\n",
    "                    fog_of_war=FogOfWarConfig(\n",
    "                        draw=False,\n",
    "                        visibility_dist=5.0,\n",
    "                        fov=90,\n",
    "                    ),\n",
    "                ),\n",
    "                \"collisions\": CollisionsMeasurementConfig(),\n",
    "            }\n",
    "        )\n",
    "dataset = habitat.make_dataset(\n",
    "        id_dataset=\"OfflineDataset-v1\", config=config.habitat.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d992365a-d0c3-4abe-9cb3-07ee763e6bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5388/5388 [00:07<00:00, 757.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# episode dictionary\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "episodes_dict = {(episode.scene_id, episode.episode_id): episode for episode in dataset.episodes}\n",
    "episodes = []\n",
    "\n",
    "class OfflineTrajectory(NavigationEpisode):\n",
    "    def __init__(self, base_episode, actions, trajectory_id):\n",
    "        # Initialize the base NavigationEpisode class with attributes from the base episode\n",
    "        super().__init__(\n",
    "            episode_id=base_episode.episode_id,\n",
    "            scene_id=base_episode.scene_id,\n",
    "            scene_dataset_config=base_episode.scene_dataset_config,\n",
    "            additional_obj_config_paths=base_episode.additional_obj_config_paths,\n",
    "            start_position=base_episode.start_position,\n",
    "            start_rotation=base_episode.start_rotation,\n",
    "            info=base_episode.info,\n",
    "            goals=base_episode.goals,\n",
    "            start_room=base_episode.start_room,\n",
    "            shortest_paths=base_episode.shortest_paths\n",
    "        )\n",
    "\n",
    "        # Add the actions variable to the class\n",
    "        self.actions = actions\n",
    "        self.trajectory_id = trajectory_id\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single file to extract scene_id and episode_id.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith(\".pkl\"):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        elif file_path.endswith(\".pt\"):\n",
    "            data = torch.load(file_path)\n",
    "        return data.get('action'), data.get(\"scene_id\"), data.get(\"episode_id\"), file_path.split(\".\")[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "\n",
    "# Initialize a multiprocessing pool\n",
    "directory = \"/srv/flash1/pputta7/projects/lm-nav/data/datasets/lmnav/offline_1env_karmesh_multipath\"\n",
    "pool = multiprocessing.Pool(processes=16)\n",
    "\n",
    "# go through each trajectory and create a navigation episode\n",
    "# Create a list of file paths to process\n",
    "file_paths = [os.path.join(root, file) for root, dirs, files in os.walk(directory) \n",
    "              for file in files if file.startswith(\"data\") and (file.endswith(\".pt\") or file.endswith(\".pkl\"))][:]\n",
    "print(len(file_paths))\n",
    "\n",
    "\n",
    "# Process files in parallel with a progress bar\n",
    "for actions, scene_id, episode_id, trajectory_id in tqdm(pool.imap_unordered(process_file, file_paths), total=len(file_paths)):\n",
    "    try:\n",
    "        base_episode = episodes_dict[(scene_id, episode_id)]\n",
    "        episodes.append(OfflineTrajectory(base_episode, actions.tolist(), trajectory_id))\n",
    "    except:\n",
    "        print(\"error\", file_path)\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62b9f7de-cd8a-47af-9fea-66043e184338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished generating data for scene /srv/flash1/pputta7/projects/lm-nav/data/datasets/lmnav_episodes/offline_1env_karmesh_multipath.json.gz\n"
     ]
    }
   ],
   "source": [
    "new_dataset = habitat.Dataset()\n",
    "new_dataset.episodes = episodes\n",
    "\n",
    "new_dirpath = f\"/srv/flash1/pputta7/projects/lm-nav/data/datasets/lmnav_episodes/\"\n",
    "os.makedirs(new_dirpath, exist_ok=True)\n",
    "new_path = os.path.join(new_dirpath, f\"{os.path.basename(directory)}.json.gz\")\n",
    "\n",
    "with gzip.open(new_path, 'wb+') as f:\n",
    "    f.write(str.encode(new_dataset.to_json()))\n",
    "    print(f'finished generating data for scene {new_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad0815ac-3268-4d85-ba3a-15dd26a97a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OfflineTrajectory(episode_id=3892, scene_id='data/scene_datasets/hm3d/train/00744-1S7LAXRdDqK/1S7LAXRdDqK.basis.glb', scene_dataset_config='default', additional_obj_config_paths=[], start_position=[-8.531359672546387, 0.19160032272338867, -6.199461460113525], start_rotation=[0.0, 0.545838794638867, 0.0, -0.8378902137316014], info={'geodesic_distance': 2.936727523803711, 'difficulty': 'easy'}, _shortest_path_cache=None, goals=[NavigationGoal(position=[-9.719176292419434, 0.19160032272338867, -3.5400874614715576], radius=None)], start_room=None, shortest_paths=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27c431b7-ce73-492c-abf3-59b35aab9160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offline_1env_karmesh_multipath'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c5247-6959-40f9-a305-027b205543be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
