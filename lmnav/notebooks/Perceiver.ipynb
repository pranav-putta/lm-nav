{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f375ac-3b8e-4cc9-8631-63fb14a6f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-10 12:53:42,374] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from lmnav.models.perceiver import Perceiver\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a01729-904b-461e-99dc-49e017b1862a",
   "metadata": {},
   "source": [
    "## Perceiver Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "696938f0-51bb-4543-8327-b19f504572c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceiver = Perceiver(\n",
    "    input_channels = 768,          # number of channels for each token of the input\n",
    "    input_axis = 1,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 64,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 16384.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    depth = 2,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "                                 #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "    num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    latent_dim = 768,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 128,         # number of dimensions per cross attention head\n",
    "    latent_dim_head = 128,        # number of dimensions per latent self attention head\n",
    "    final_classifier_head = False,          # output number of classes\n",
    "    attn_dropout = 0.1,\n",
    "    ff_dropout = 0.1,\n",
    "    weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "    fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "    self_per_cross_attn = 7      # number of self attention blocks per cross attention\n",
    ")\n",
    "perceiver = perceiver.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ad37a-af10-4d33-af51-2981b6073b3b",
   "metadata": {},
   "source": [
    "## LLAMA Forward with Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4a0bea-b010-435e-907c-75a8301c327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from lmnav.common.config import Config\n",
    "from lmnav.common.registry import registry\n",
    "\n",
    "from lmnav.models import *\n",
    "from lmnav.processors import *\n",
    "from lmnav.common.episode_processor import apply_transforms_inputs\n",
    "\n",
    "import torch\n",
    "import einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55765275-f64c-4188-b912-aeaffdb17d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_components(cfg_path, device):\n",
    "    Args = namedtuple(\"Args\", \"cfg_path, model_type, gpu_id, options\")\n",
    "    args = Args(cfg_path, \"llama_v2\", 0, [])\n",
    "\n",
    "    cfg = Config(args)\n",
    "\n",
    "    model_config = cfg.model_cfg\n",
    "    model_cls = registry.get_model_class(model_config.arch)\n",
    "    model = model_cls.from_config(model_config).to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    vis_processor_cfg = cfg.config.preprocess.vis_processor.train\n",
    "    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "\n",
    "    return model, vis_processor\n",
    "\n",
    "\n",
    "def test_construct_inputs(B, T):\n",
    "    goals = torch.rand(B, 1, 3, 480, 640)\n",
    "    rgbs = torch.rand(B, T, 3, 480, 640)\n",
    "    actions = torch.randint(0, 4, (B, T))\n",
    "\n",
    "    return goals, rgbs, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83041ac7-6c35-4e7f-9887-0109a45bdc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db520a54f3e499391d37107859f4e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg_path = \"/srv/flash1/pputta7/projects/lm-nav/exp_configs/lora_nav_llama_train.yaml\"\n",
    "device = 'cuda'\n",
    "B, T = 2, 20\n",
    "\n",
    "model, vis_processor = _init_components(cfg_path, device)\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ea563-a4bd-47ce-976b-ddda3bcd5a54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Perceiver Mask Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fe54ba-7226-4dce-b9f7-71a9096c7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100, 3, 224, 224).to('cuda').half()\n",
    "embds = model.visual_encoder(x)\n",
    "qtk = model.query_tokens.expand(embds.shape[0], -1, -1)\n",
    "out = model.Qformer.bert(query_embeds = qtk,\n",
    "                         encoder_hidden_states=embds,\n",
    "                         return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9c696e-bd45-4ae4-8e73-7a04eb8ee4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4a77fc-9d97-4351-9542-0901a774911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "122a480a-2e09-4587-99c3-dc2b9c9e3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, D = 1, 1, 512\n",
    "Nkv = 5\n",
    "Nq = 3\n",
    "q = torch.rand(B, Nq, D)\n",
    "k, v = (torch.rand(B, Nkv, D) for _ in range(2))\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=H), (q, k, v))\n",
    "sim = einsum('b i d, b j d -> b i j', q, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d697dd5-1e85-4a3a-b721-3a2456ee649c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65bac106-179c-4a2e-b436-9c1b8212b177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True, False]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones(B, Nkv)\n",
    "mask[..., -1] = 0\n",
    "mask = rearrange(mask, 'b ... -> b (...)')\n",
    "neg = -torch.finfo(sim.dtype).max\n",
    "mask = repeat(mask, 'b j -> (b h) () j', h=H)\n",
    "mask = mask.to(torch.bool)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f5107f8-b8a8-421f-9d37-5e1cfedafbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3009e+02,  1.2752e+02,  1.2684e+02,  1.3022e+02, -3.4028e+38],\n",
       "         [ 1.2227e+02,  1.1984e+02,  1.2209e+02,  1.2447e+02, -3.4028e+38],\n",
       "         [ 1.3540e+02,  1.2980e+02,  1.2632e+02,  1.2921e+02, -3.4028e+38]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.masked_fill_(~mask, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191bb81-ae8d-4e52-9799-9a4c83d6ee64",
   "metadata": {},
   "source": [
    "### Prompt Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469501a0-33db-489d-a23c-673a1438a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a navigational agent tasked with exploring an indoor environment to find a goal image. \\\n",
    "                  You can choose to move { left, right, forward, stop } at every step. The goal image is {}. \\\n",
    "                  You are provided with a video of your state action history: {}. Choose the next best action: {}\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f32905a-5ed2-47d4-9872-527979d3f274",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Name 'lin_nav_llama' already registered for <class 'lmnav.models.lin_nav_llama.LinNavLLAMA'>.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, repeat\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlmnav\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo_nav_llama\u001b[39;00m\n\u001b[1;32m      4\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(lmnav\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mvideo_nav_llama)\n\u001b[1;32m      6\u001b[0m B, T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m/coc/testnvme/pputta7/projects/lm-nav/lmnav/models/video_nav_llama.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_config, get_peft_model, LoraConfig, TaskType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# from flamingo_pytorch import PerceiverResampler\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@registry\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlin_nav_llama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mLinNavLLAMA\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBlip2Base\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43;03m    Extension from BLIP2 GPT-LLAMA model to operate over navigation space.\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43;03m    Adds a linear transformation to the BLIP projections.\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPRETRAINED_MODEL_CONFIG_DICT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretrain_vicuna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigs/models/video_llama.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretrain_llama_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigs/models/video_llama.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[0;32m/coc/testnvme/pputta7/projects/lm-nav/lmnav/common/registry.py:101\u001b[0m, in \u001b[0;36mRegistry.register_model.<locals>.wrap\u001b[0;34m(model_cls)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\n\u001b[1;32m     98\u001b[0m     model_cls, BaseModel\n\u001b[1;32m     99\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll models must inherit BaseModel class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already registered for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    103\u001b[0m             name, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m][name]\n\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m][name] \u001b[38;5;241m=\u001b[39m model_cls\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_cls\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Name 'lin_nav_llama' already registered for <class 'lmnav.models.lin_nav_llama.LinNavLLAMA'>.\""
     ]
    }
   ],
   "source": [
    "from einops import rearrange, repeat\n",
    "import importlib\n",
    "import lmnav.models.video_nav_llama\n",
    "importlib.reload(lmnav.models.video_nav_llama)\n",
    "\n",
    "B, T = 2, 10\n",
    "\n",
    "rgbs = torch.rand(B, 3, T, 224, 224)\n",
    "goals = torch.rand(B, 3, 1, 224, 224)\n",
    "actions = torch.randint(0, 4, (B, 1))\n",
    "mask = torch.ones(B, T).bool()\n",
    "\n",
    "rgbs_embd, rgbs_attn = model.embed_visual(rgbs)\n",
    "goals_embd, goals_attn = model.embed_visual(goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c7fe125-64a0-4af8-ac8b-332273b20f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the perceiver sequence: [goal] + [states]\n",
    "video = torch.cat([goals_embd, rgbs_embd], dim=1)\n",
    "video = rearrange(video, 'b t n h -> b (t n) h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e89c8c49-6a84-456a-afe0-e426f603444a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 352, 4096])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f198069-edcc-42fe-ae99-bfaec77d10ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 4096])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video[:, :32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ea3b9-b25a-4ecb-b693-79a387c8d268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
