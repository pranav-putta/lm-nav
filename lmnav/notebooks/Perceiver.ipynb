{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f375ac-3b8e-4cc9-8631-63fb14a6f4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-10 13:26:00,146] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from lmnav.models.perceiver import Perceiver\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a01729-904b-461e-99dc-49e017b1862a",
   "metadata": {},
   "source": [
    "## Perceiver Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696938f0-51bb-4543-8327-b19f504572c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceiver = Perceiver(\n",
    "    input_channels = 768,          # number of channels for each token of the input\n",
    "    input_axis = 1,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 64,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 16384.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    depth = 2,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "                                 #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "    num_latents = 256,           # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    latent_dim = 768,            # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 8,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 128,         # number of dimensions per cross attention head\n",
    "    latent_dim_head = 128,        # number of dimensions per latent self attention head\n",
    "    final_classifier_head = False,          # output number of classes\n",
    "    attn_dropout = 0.1,\n",
    "    ff_dropout = 0.1,\n",
    "    weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "    fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "    self_per_cross_attn = 7      # number of self attention blocks per cross attention\n",
    ")\n",
    "perceiver = perceiver.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ad37a-af10-4d33-af51-2981b6073b3b",
   "metadata": {},
   "source": [
    "## LLAMA Forward with Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4a0bea-b010-435e-907c-75a8301c327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from lmnav.common.config import Config\n",
    "from lmnav.common.registry import registry\n",
    "\n",
    "from lmnav.models import *\n",
    "from lmnav.processors import *\n",
    "from lmnav.common.episode_processor import apply_transforms_inputs\n",
    "\n",
    "import torch\n",
    "import einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55765275-f64c-4188-b912-aeaffdb17d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_components(cfg_path, device):\n",
    "    Args = namedtuple(\"Args\", \"cfg_path, model_type, gpu_id, options\")\n",
    "    args = Args(cfg_path, \"llama_v2\", 0, [])\n",
    "\n",
    "    cfg = Config(args)\n",
    "\n",
    "    model_config = cfg.model_cfg\n",
    "    model_cls = registry.get_model_class(model_config.arch)\n",
    "    model = model_cls.from_config(model_config).to(device)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    vis_processor_cfg = cfg.config.preprocess.vis_processor.train\n",
    "    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "\n",
    "    return model, vis_processor\n",
    "\n",
    "\n",
    "def test_construct_inputs(B, T):\n",
    "    goals = torch.rand(B, 1, 3, 480, 640)\n",
    "    rgbs = torch.rand(B, T, 3, 480, 640)\n",
    "    actions = torch.randint(0, 4, (B, T))\n",
    "\n",
    "    return goals, rgbs, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83041ac7-6c35-4e7f-9887-0109a45bdc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858c3ef2e92047af96b37cba2a2f1e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg_path = \"/srv/flash1/pputta7/projects/lm-nav/exp_configs/video_nav_llama_train.yaml\"\n",
    "device = 'cuda'\n",
    "B, T = 2, 20\n",
    "\n",
    "model, vis_processor = _init_components(cfg_path, device)\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ea563-a4bd-47ce-976b-ddda3bcd5a54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Perceiver Mask Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74fe54ba-7226-4dce-b9f7-71a9096c7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100, 3, 224, 224).to('cuda').half()\n",
    "embds = model.visual_encoder(x)\n",
    "qtk = model.query_tokens.expand(embds.shape[0], -1, -1)\n",
    "out = model.Qformer.bert(query_embeds = qtk,\n",
    "                         encoder_hidden_states=embds,\n",
    "                         return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9c696e-bd45-4ae4-8e73-7a04eb8ee4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4a77fc-9d97-4351-9542-0901a774911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "122a480a-2e09-4587-99c3-dc2b9c9e3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, H, D = 1, 1, 512\n",
    "Nkv = 5\n",
    "Nq = 3\n",
    "q = torch.rand(B, Nq, D)\n",
    "k, v = (torch.rand(B, Nkv, D) for _ in range(2))\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=H), (q, k, v))\n",
    "sim = einsum('b i d, b j d -> b i j', q, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d697dd5-1e85-4a3a-b721-3a2456ee649c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65bac106-179c-4a2e-b436-9c1b8212b177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  True, False]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones(B, Nkv)\n",
    "mask[..., -1] = 0\n",
    "mask = rearrange(mask, 'b ... -> b (...)')\n",
    "neg = -torch.finfo(sim.dtype).max\n",
    "mask = repeat(mask, 'b j -> (b h) () j', h=H)\n",
    "mask = mask.to(torch.bool)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f5107f8-b8a8-421f-9d37-5e1cfedafbf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3009e+02,  1.2752e+02,  1.2684e+02,  1.3022e+02, -3.4028e+38],\n",
       "         [ 1.2227e+02,  1.1984e+02,  1.2209e+02,  1.2447e+02, -3.4028e+38],\n",
       "         [ 1.3540e+02,  1.2980e+02,  1.2632e+02,  1.2921e+02, -3.4028e+38]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.masked_fill_(~mask, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191bb81-ae8d-4e52-9799-9a4c83d6ee64",
   "metadata": {},
   "source": [
    "### Prompt Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469501a0-33db-489d-a23c-673a1438a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a navigational agent tasked with exploring an indoor environment to find a goal image. \\\n",
    "                  You can choose to move { left, right, forward, stop } at every step. The goal image is {}. \\\n",
    "                  You are provided with a video of your state action history: {}. Choose the next best action: {}\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f32905a-5ed2-47d4-9872-527979d3f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "import importlib\n",
    "\n",
    "B, T = 2, 10\n",
    "\n",
    "rgbs = torch.rand(B, 3, T, 224, 224)\n",
    "goals = torch.rand(B, 3, 1, 224, 224)\n",
    "actions = [['left'], ['forward']]\n",
    "\n",
    "rgbs_embd = model.embed_visual(rgbs)\n",
    "goals_embd = model.embed_visual(goals)\n",
    "rgbs_embd = rearrange(rgbs_embd, '(b t) ... -> b t ...', b=B)\n",
    "goals_embd = rearrange(goals_embd, '(b t) ... -> b t ...', b=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cce0adbe-4514-4b4c-826e-39e19331f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 4096])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones(B, T + 1).bool().to('cuda')\n",
    "mask = repeat(mask, 'b t -> b (t n)', n=32)\n",
    "\n",
    "# construct the perceiver sequence: [goal] + [states]\n",
    "video = torch.cat([goals_embd, rgbs_embd], dim=1)\n",
    "video = rearrange(video, 'b t n h -> b (t n) h').half()\n",
    "vid_embds = model.vidformer(video, mask=mask)\n",
    "vid_embds = model.llama_proj(vid_embds)\n",
    "\n",
    "vid_embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f198069-edcc-42fe-ae99-bfaec77d10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_segs = prompt.split(\"{}\")\n",
    "prompt_tkns = [model.llama_tokenizer(seg, return_tensors='pt', add_special_tokens=(i == 0)) for i, seg in enumerate(prompt_segs) if len(seg)]\n",
    "prompt_embd = [model.llama_model.get_input_embeddings()(tkns.input_ids.to(model.device)) for tkns in prompt_tkns]\n",
    "prompt_embd = [embd.repeat(B, 1, 1) for embd in prompt_embd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "546ea3b9-b25a-4ecb-b693-79a387c8d268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>You are a navigational agent tasked with exploring an indoor environment to find a goal image.                   You can choose to move { left, right, forward, stop } at every step. The goal image is ',\n",
       " '.                   You are provided with a video of your state action history: ',\n",
       " '. Choose the next best action: ']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.llama_tokenizer.batch_decode([e['input_ids'][0] for e in prompt_tkns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6ce609ab-2ca7-4734-a5c4-0a1bfed98248",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_tkns_t = model.tokenize_actions(actions)\n",
    "actions_embd = model.embed_actions(action_tkns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a44de9ce-bb01-4118-bd4a-dc5adcd16f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "goals_embd = model.llama_proj(goals_embd.half())\n",
    "goals_embd = goals_embd.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "851e2980-6e66-4dfe-baed-33492c0f7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "embds = [prompt_embd[0], goals_embd, prompt_embd[1], vid_embds, prompt_embd[2], actions_embd] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3371a3a4-fb76-49a9-ba9f-b0f01b20909e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 47, 4096]),\n",
       " torch.Size([2, 32, 4096]),\n",
       " torch.Size([2, 16, 4096]),\n",
       " torch.Size([2, 256, 4096]),\n",
       " torch.Size([2, 9, 4096]),\n",
       " torch.Size([2, 1, 4096])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210b914-4ee8-4de7-96be-2761f2f90ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
