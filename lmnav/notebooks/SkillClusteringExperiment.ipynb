{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7737772c-3df3-4a08-b191-cf5a0aad066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import habitat\n",
    "import gzip\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import RelaxedOneHotCategorical, kl_divergence\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "255488a8-3541-428a-8334-e3a837951990",
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [ [1] * 12, [1, 2] * 15, [2, 3] * 12 + [1] ]\n",
    "mode2class = { sum(modes[i]): i for i in range(len(modes)) }\n",
    "class ActionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path=None, bostoken=4, min_len=8, max_len=40):\n",
    "        assert (data_path is not None)\n",
    "        \n",
    "        # set metadata using the config file\n",
    "        self.files = [os.path.join(data_path, file) for file in sorted(os.listdir(data_path))]\n",
    "        self.bostoken = bostoken\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        # episode = torch.load(file)\n",
    "        # # with gzip.open(file) as f:\n",
    "        #     # episode = pickle.load(f)\n",
    "\n",
    "        # # add a BOS token\n",
    "        # actions = episode['action'].tolist()\n",
    "        # actions = [self.bostoken] + actions\n",
    "        # rand_len = min(randrange(self.min_len, self.max_len), len(actions)) \n",
    "        # start = randrange(0, len(actions) - rand_len + 1)\n",
    "        # actions = actions[start:start + rand_len]\n",
    "        # idx = randrange(0, 3)\n",
    "        return torch.tensor(modes[idx % 3], dtype=torch.long)\n",
    "        # return torch.tensor(actions, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47817261-154a-4c6e-a4bd-3742798b1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU Unit\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Linear Layer for Mean and Variance\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim * 2)  # Multiply by 2 for mean and variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input sequence of discrete values\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # GRU layer\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        # Linear layer for mean and variance\n",
    "        output = self.linear(hidden[-1])  # Taking the output of the last time step\n",
    "        \n",
    "        # Split the output into mean and variance\n",
    "        mean, log_var = torch.chunk(output, 2, dim=-1)\n",
    "        \n",
    "        return mean, log_var\n",
    "\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(latent_dim, len(modes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear(x))\n",
    "        \n",
    "\n",
    "class SequenceDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers, vocab_size, embedding_module):\n",
    "        super(SequenceDecoder, self).__init__()\n",
    "        self.nlayers = num_layers\n",
    "\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(embedding_module.embedding_dim, latent_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Linear layer to output continuous values\n",
    "        self.linear = nn.Linear(latent_dim, vocab_size)\n",
    "        self.embedding_module = embedding_module\n",
    "\n",
    "    def sample_sequence(self, length, start_token, temperature=1.0, hidden=None):\n",
    "        inp = torch.tensor([start_token] * hidden.shape[1], dtype=torch.long, device='cuda')[:, None]\n",
    "        output_sequence = []\n",
    "        for _ in range(length):\n",
    "            inp = self.embedding_module(inp)\n",
    "            out, hidden = self.gru(inp, hidden)\n",
    "            logits = self.linear(out)\n",
    "            logits = logits / temperature\n",
    "            probabilities = F.softmax(logits.squeeze(), dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            output_sequence.append(probabilities)\n",
    "            inp = next_token.long()[:, None]\n",
    "        return torch.stack(output_sequence, dim=0)[None,:]\n",
    "\n",
    "\n",
    "    def forward(self, z, seq_length):\n",
    "        # 'z' is the latent vector\n",
    "        # 'seq_length' is the desired length of the generated sequence\n",
    "        # 'batch_size' is the batch size\n",
    "        hx = z.expand(self.nlayers, z.shape[0], -1).contiguous()\n",
    "        return self.sample_sequence(seq_length, padtoken, hidden=hx)\n",
    "\n",
    "\n",
    "def sample_from_latent_space(mean, log_var):\n",
    "    \"\"\"\n",
    "    Samples from the latent space using the reparameterization trick.\n",
    "\n",
    "    Args:\n",
    "        mean (torch.Tensor): Mean values from the encoder.\n",
    "        log_var (torch.Tensor): Log variance values from the encoder.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Samples from the latent space.\n",
    "    \"\"\"\n",
    "    # Standard deviation (sigma) is the square root of the variance (exp(log_var/2))\n",
    "    std_dev = torch.exp(0.5 * log_var)\n",
    "\n",
    "    # Sample from a standard normal distribution\n",
    "    epsilon = torch.randn_like(std_dev)\n",
    "\n",
    "    # Reparameterization trick to sample from the latent space\n",
    "    sampled_latent = mean + std_dev * epsilon\n",
    "\n",
    "    return sampled_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47b85843-a9c2-40e3-a65d-534b48f5394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(output_sequence, target_sequence, mean, log_var, mask):\n",
    "    # Flatten the sequences and masks for convenience\n",
    "    output_sequence = output_sequence.view(-1, output_sequence.size(-1))\n",
    "    target_sequence = target_sequence.view(-1)\n",
    "    mask = mask.view(-1)\n",
    "    \n",
    "    # Reconstruction loss using cross-entropy loss (ignoring padding tokens)\n",
    "    reconstruction_loss = torch.sum(\n",
    "        F.cross_entropy(output_sequence, target_sequence, reduction='none') #* mask\n",
    "    ) / torch.sum(mask)\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_divergence_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    \n",
    "    # Total VAE loss\n",
    "    \n",
    "    return reconstruction_loss, kl_divergence_loss\n",
    "    \n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, latent_dim, layers):\n",
    "        super().__init__()\n",
    "        self.encoder = SequenceEncoder(vocab_size=6, embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_layers=layers, output_dim=latent_dim)\n",
    "        self.decoder = SequenceDecoder(vocab_size=6, latent_dim=latent_dim, hidden_dim=hidden_dim, num_layers=layers, embedding_module=self.encoder.embedding)\n",
    "        self.classifier = SequenceClassifier(latent_dim=latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        mean, var = self.encoder(x)\n",
    "        z = sample_from_latent_space(mean, var)\n",
    "        # out = self.decoder(z, seq_length=T)\n",
    "        out = self.classifier(z)\n",
    "        return mean, var, out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77a06a0b-52e5-486a-8dfb-c5581f6d1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seq_length': None,\n",
    "    'feature_size': 2,\n",
    "    'batch_size': 64,\n",
    "    'workers': 16,\n",
    "    'epochs': 1,\n",
    "    'lr': 1e-3,\n",
    "\n",
    "    # model architecture\n",
    "    'embedding': 4,\n",
    "    'hidden': 8,\n",
    "    'latent_dim': 64,\n",
    "    'layers': 4\n",
    "}\n",
    "bostoken=4\n",
    "padtoken=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f999beda-791b-4dc5-a7f9-287dab4f9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params 91713\n"
     ]
    }
   ],
   "source": [
    "dataset = ActionDataset(\"/srv/flash1/pputta7/projects/lm-nav/data/datasets/lmnav/offline_10envs/\")\n",
    "dataloader = DataLoader(dataset, collate_fn=lambda t: pad_sequence(t, batch_first=True, padding_value=padtoken), batch_size=config['batch_size'], num_workers=config['workers'])\n",
    "model = Model(config['embedding'], config['hidden'], config['latent_dim'], config['layers'])\n",
    "print(\"Num params\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6048e52-b568-4991-8b29-c40f132d7b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                              | 0/714 [00:00<?, ?it/s]/tmp/ipykernel_130129/3163995448.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.linear(x))\n",
      "Avg Loss: 10.696387437941814 ; R: 0.049399474093100935 ; K: 10.646987920966135: 100%|█| 714\n",
      "Avg Loss: 0.04939724425120013 ; R: 0.04925078407702159 ; K: 0.00014646017417854288: 100%|█|\n",
      "Avg Loss: 0.04926701729578712 ; R: 0.049240853540352174 ; K: 2.6163755434949252e-05:  59%|▌\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m mask \u001b[38;5;241m=\u001b[39m (actions \u001b[38;5;241m!=\u001b[39m padtoken)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool) \n\u001b[1;32m     17\u001b[0m mean, var, pred_actions \u001b[38;5;241m=\u001b[39m model(actions)\n\u001b[0;32m---> 19\u001b[0m actlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([mode2class[\u001b[38;5;28msum\u001b[39m(actions[i] \u001b[38;5;241m*\u001b[39m mask[i])\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m reconstruction_loss, kl_divergence_loss \u001b[38;5;241m=\u001b[39m vae_loss(pred_actions, actlabels, mean, var, mask)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m  kl_divergence_loss\n",
      "Cell \u001b[0;32mIn[66], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m mask \u001b[38;5;241m=\u001b[39m (actions \u001b[38;5;241m!=\u001b[39m padtoken)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool) \n\u001b[1;32m     17\u001b[0m mean, var, pred_actions \u001b[38;5;241m=\u001b[39m model(actions)\n\u001b[0;32m---> 19\u001b[0m actlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([mode2class[\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(actions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m reconstruction_loss, kl_divergence_loss \u001b[38;5;241m=\u001b[39m vae_loss(pred_actions, actlabels, mean, var, mask)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m  kl_divergence_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "model = model.cuda()\n",
    "\n",
    "ckpt = torch.load('skill_vae.pt')\n",
    "# model.load_state_dict(ckpt['model'])\n",
    "# optim.load_state_dict(ckpt['optim'])\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    rloss = 0\n",
    "    klloss = 0\n",
    "    steps = 0\n",
    "    for actions in (pbar := tqdm(dataloader)):\n",
    "        actions = actions.to('cuda')\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        mask = (actions != padtoken).clone().to(torch.bool) \n",
    "        mean, var, pred_actions = model(actions)\n",
    "\n",
    "        actlabels = torch.tensor([mode2class[sum(actions[i] * mask[i]).item()] for i in range(actions.shape[0])]).cuda()\n",
    "        \n",
    "        reconstruction_loss, kl_divergence_loss = vae_loss(pred_actions, actlabels, mean, var, mask)\n",
    "        loss = reconstruction_loss +  kl_divergence_loss\n",
    "        \n",
    "        total_loss += loss.cpu().item()\n",
    "        rloss += reconstruction_loss.cpu().item()\n",
    "        klloss += kl_divergence_loss.cpu().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        pbar.set_description(f\"Avg Loss: {total_loss / steps} ; R: {rloss / steps} ; K: {klloss / steps}\")\n",
    "        if steps % 25 == 0:\n",
    "            torch.save({'model': model.state_dict(), 'optim': optim.state_dict()}, 'skill_vae2.pt')\n",
    "            # print(pred_actions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "012d9de8-b1e3-40f4-b70a-7a7d29bba6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params 91713\n"
     ]
    }
   ],
   "source": [
    "dataset = ActionDataset(\"/srv/flash1/pputta7/projects/lm-nav/data/datasets/lmnav/offline_10envs/\")\n",
    "dataloader = DataLoader(dataset, collate_fn=lambda t: pad_sequence(t, batch_first=True, padding_value=padtoken), batch_size=1, num_workers=1)\n",
    "model = Model(config['embedding'], config['hidden'], config['latent_dim'], config['layers']).cuda()\n",
    "model.load_state_dict(torch.load('skill_vae2.pt')['model'])\n",
    "print(\"Num params\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5bf4d8b-e06f-4539-8f37-e97f1d77b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/45650 [00:00<?, ?it/s]/tmp/ipykernel_130129/3163995448.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.linear(x))\n",
      "  0%|                                                  | 1/45650 [00:00<1:47:03,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3188, 0.3741, 0.3071]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.3100, 0.3390, 0.3510]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.4400, 0.2721, 0.2878]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3397, 0.3354, 0.3249]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.3314, 0.4060, 0.2626]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.3068, 0.3993, 0.2939]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3349, 0.3653, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.3458, 0.3545, 0.2998]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.4320, 0.2641, 0.3039]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3522, 0.3182, 0.3296]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.2824, 0.3719, 0.3457]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.3189, 0.3725, 0.3087]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3900, 0.3283, 0.2816]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.2489, 0.3177, 0.4334]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.3597, 0.3061, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3597, 0.3411, 0.2992]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.2669, 0.3287, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.3705, 0.3369, 0.2926]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3561, 0.3560, 0.2880]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.2882, 0.3122, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.3429, 0.3287, 0.3284]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n",
      "tensor([[0.3675, 0.3602, 0.2723]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 0\n",
      "tensor([[0.3159, 0.3337, 0.3503]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 1\n",
      "tensor([[0.3380, 0.3515, 0.3105]], device='cuda:0', grad_fn=<SoftmaxBackward0>) 2\n"
     ]
    }
   ],
   "source": [
    "embeds = []\n",
    "for i, actions in (pbar := enumerate(tqdm(dataloader))):\n",
    "    mean, var, pred_actions = model(actions.cuda())\n",
    "    print(pred_actions, mode2class[actions.sum().item()])\n",
    "    if i == 23:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959b4d4b-d231-42df-9372-561882467e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
      "         1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(pred_actions, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c6ac01d-6e8f-4ef1-a223-a72a1b892bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n",
       "         1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 18/45650 [00:20<13:06, 58.00it/s]"
     ]
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c697e49-b7b3-459b-b30a-c54204b7978f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.3743e-06, 9.9899e-01, 9.8306e-04, 9.5443e-06, 6.7561e-06,\n",
       "          8.1095e-06],\n",
       "         [6.0537e-07, 9.9985e-01, 1.4796e-04, 2.8720e-07, 6.5884e-07,\n",
       "          7.8119e-07],\n",
       "         [4.0390e-07, 9.9992e-01, 8.1917e-05, 9.9207e-08, 4.4757e-07,\n",
       "          5.0765e-07],\n",
       "         [3.6830e-07, 9.9994e-01, 6.2958e-05, 5.9764e-08, 4.1627e-07,\n",
       "          4.6077e-07],\n",
       "         [3.6063e-07, 9.9994e-01, 5.4598e-05, 4.3995e-08, 4.1177e-07,\n",
       "          4.5053e-07],\n",
       "         [3.6005e-07, 9.9995e-01, 5.0083e-05, 3.5629e-08, 4.1325e-07,\n",
       "          4.4904e-07],\n",
       "         [3.6141e-07, 9.9995e-01, 4.7332e-05, 3.0450e-08, 4.1625e-07,\n",
       "          4.4986e-07],\n",
       "         [3.6318e-07, 9.9995e-01, 4.5525e-05, 2.6928e-08, 4.1953e-07,\n",
       "          4.5123e-07],\n",
       "         [3.6484e-07, 9.9995e-01, 4.4285e-05, 2.4373e-08, 4.2272e-07,\n",
       "          4.5260e-07],\n",
       "         [3.6624e-07, 9.9996e-01, 4.3413e-05, 2.2430e-08, 4.2566e-07,\n",
       "          4.5376e-07],\n",
       "         [3.6731e-07, 9.9996e-01, 4.2796e-05, 2.0892e-08, 4.2827e-07,\n",
       "          4.5462e-07],\n",
       "         [3.6803e-07, 9.9996e-01, 4.2361e-05, 1.9633e-08, 4.3050e-07,\n",
       "          4.5515e-07],\n",
       "         [3.6846e-07, 9.9996e-01, 4.2060e-05, 1.8577e-08, 4.3240e-07,\n",
       "          4.5538e-07],\n",
       "         [3.6870e-07, 9.9996e-01, 4.1853e-05, 1.7678e-08, 4.3408e-07,\n",
       "          4.5543e-07],\n",
       "         [3.6891e-07, 9.9996e-01, 4.1713e-05, 1.6905e-08, 4.3566e-07,\n",
       "          4.5543e-07],\n",
       "         [3.6914e-07, 9.9996e-01, 4.1625e-05, 1.6227e-08, 4.3722e-07,\n",
       "          4.5544e-07],\n",
       "         [3.6930e-07, 9.9996e-01, 4.1589e-05, 1.5607e-08, 4.3870e-07,\n",
       "          4.5535e-07],\n",
       "         [3.6926e-07, 9.9996e-01, 4.1622e-05, 1.5002e-08, 4.4000e-07,\n",
       "          4.5501e-07],\n",
       "         [3.6928e-07, 9.9996e-01, 4.1723e-05, 1.4404e-08, 4.4145e-07,\n",
       "          4.5475e-07],\n",
       "         [3.7009e-07, 9.9996e-01, 4.1860e-05, 1.3844e-08, 4.4374e-07,\n",
       "          4.5545e-07],\n",
       "         [3.7196e-07, 9.9996e-01, 4.2024e-05, 1.3376e-08, 4.4703e-07,\n",
       "          4.5747e-07],\n",
       "         [3.7415e-07, 9.9996e-01, 4.2313e-05, 1.3045e-08, 4.5016e-07,\n",
       "          4.6005e-07],\n",
       "         [3.7369e-07, 9.9996e-01, 4.3129e-05, 1.2935e-08, 4.4923e-07,\n",
       "          4.6011e-07],\n",
       "         [3.6466e-07, 9.9995e-01, 4.5688e-05, 1.3314e-08, 4.3747e-07,\n",
       "          4.5205e-07],\n",
       "         [3.4148e-07, 9.9995e-01, 5.3107e-05, 1.4748e-08, 4.1116e-07,\n",
       "          4.3240e-07],\n",
       "         [3.0506e-07, 9.9993e-01, 7.0773e-05, 1.7925e-08, 3.7330e-07,\n",
       "          4.0110e-07],\n",
       "         [2.6487e-07, 9.9990e-01, 1.0386e-04, 2.3131e-08, 3.3025e-07,\n",
       "          3.6012e-07],\n",
       "         [2.3135e-07, 9.9984e-01, 1.5490e-04, 3.0118e-08, 2.9038e-07,\n",
       "          3.1846e-07],\n",
       "         [2.0849e-07, 9.9978e-01, 2.2302e-04, 3.8376e-08, 2.5914e-07,\n",
       "          2.8468e-07],\n",
       "         [1.9457e-07, 9.9969e-01, 3.0541e-04, 4.7436e-08, 2.3648e-07,\n",
       "          2.6016e-07]]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
