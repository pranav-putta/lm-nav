Traceback (most recent call last):
  File "/coc/testnvme/pputta7/projects/lm-nav/lmnav/bc_train.py", line 355, in <module>
    main()
  File "/coc/testnvme/pputta7/projects/lm-nav/lmnav/bc_train.py", line 349, in main
    trainer.train()
  File "/coc/testnvme/pputta7/projects/lm-nav/lmnav/bc_train.py", line 224, in train
    stats = self.train_epoch(epoch)
  File "/coc/testnvme/pputta7/projects/lm-nav/lmnav/bc_train.py", line 182, in train_epoch
    outputs = self.agent(rgbs_t, goals_t, actions_t)
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/coc/testnvme/pputta7/projects/lm-nav/lmnav/models/lin_nav_llama.py", line 262, in forward
    outputs = self.llama_model(inputs_embeds=embd,
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/coc/testnvme/pputta7/projects/lm-nav/lmnav/models/modeling_llama.py", line 703, in forward
    loss = loss_fct(shift_logits, shift_labels)
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/modules/loss.py", line 1174, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/coc/testnvme/pputta7/mambaforge/envs/lmnav/lib/python3.9/site-packages/torch/nn/functional.py", line 3029, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 44.39 GiB total capacity; 33.69 GiB already allocated; 50.38 MiB free; 34.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF